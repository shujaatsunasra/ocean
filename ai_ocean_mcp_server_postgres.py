#!/usr/bin/env python3
"""
FloatChat Ocean AI MCP Server - Advanced Architecture
Comprehensive ocean data analysis and visualization platform
Phase 1: Argovis Data Optimizer, Map Intelligence, Query Intelligence
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
import requests
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
from scipy import stats
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import math

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# PHASE 1 MCP MODULES - COMPLETED âœ…
# =============================================================================

class ArgovisDataOptimizerMCP:
    """Optimizes Argovis API calls and data retrieval"""
    
    def __init__(self):
        self.cache = {}
        self.query_history = []
        self.performance_metrics = {}
    
    def optimize_query_params(self, query_params: Dict) -> Dict:
        """Optimize Argovis query parameters for better performance"""
        optimized = query_params.copy()
        
        # Optimize time range to avoid 413 errors
        if 'startDate' in optimized and 'endDate' in optimized:
            start_date = datetime.fromisoformat(optimized['startDate'].replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(optimized['endDate'].replace('Z', '+00:00'))
            
            # If query spans more than 1 year, split into chunks
            if (end_date - start_date).days > 365:
                logger.info("Large time range detected, implementing chunked queries")
                return self._chunk_large_query(optimized)
        
        # Optimize polygon for global queries
        if 'polygon' in optimized:
            polygon = json.loads(optimized['polygon'])
            if self._is_global_query(polygon):
                logger.info("Global query detected, optimizing for performance")
                return self._optimize_global_query(optimized)
        
        return optimized
    
    def _chunk_large_query(self, params: Dict) -> Dict:
        """Split large time range queries into manageable chunks"""
        start_date = datetime.fromisoformat(params['startDate'].replace('Z', '+00:00'))
        end_date = datetime.fromisoformat(params['endDate'].replace('Z', '+00:00'))
        
        # Split into 6-month chunks
        chunk_days = 180
        chunks = []
        current_start = start_date
        
        while current_start < end_date:
            current_end = min(current_start + timedelta(days=chunk_days), end_date)
            chunk_params = params.copy()
            chunk_params['startDate'] = current_start.isoformat().replace('+00:00', 'Z')
            chunk_params['endDate'] = current_end.isoformat().replace('+00:00', 'Z')
            chunks.append(chunk_params)
            current_start = current_end
        
        return {'chunked': True, 'chunks': chunks}
    
    def _is_global_query(self, polygon: List) -> bool:
        """Check if polygon represents a global query"""
        if len(polygon) < 4:
            return False
        
        # Check if polygon covers most of the globe
        lons = [point[0] for point in polygon]
        lats = [point[1] for point in polygon]
        
        lon_range = max(lons) - min(lons)
        lat_range = max(lats) - min(lats)
        
        return lon_range > 300 and lat_range > 150
    
    def _optimize_global_query(self, params: Dict) -> Dict:
        """Optimize global queries by focusing on key ocean regions"""
        # Focus on major ocean basins instead of global coverage
        ocean_regions = [
            {'name': 'North Atlantic', 'bounds': [(-80, 20), (-10, 20), (-10, 70), (-80, 70)]},
            {'name': 'North Pacific', 'bounds': [(120, 20), (-120, 20), (-120, 70), (120, 70)]},
            {'name': 'South Atlantic', 'bounds': [(-60, -60), (20, -60), (20, 0), (-60, 0)]},
            {'name': 'South Pacific', 'bounds': [(-120, -60), (120, -60), (120, 0), (-120, 0)]},
            {'name': 'Indian Ocean', 'bounds': [(20, -60), (120, -60), (120, 30), (20, 30)]}
        ]
        
        return {'regional_approach': True, 'regions': ocean_regions}
    
    def cache_query_result(self, query_key: str, result: Any) -> None:
        """Cache query results for performance"""
        self.cache[query_key] = {
            'result': result,
            'timestamp': datetime.now(),
            'access_count': 0
        }
    
    def get_cached_result(self, query_key: str) -> Optional[Any]:
        """Retrieve cached query result"""
        if query_key in self.cache:
            self.cache[query_key]['access_count'] += 1
            return self.cache[query_key]['result']
        return None

class InteractiveMapIntelligenceMCP:
    """Smart map-based analysis and interaction"""
    
    def __init__(self):
        self.ocean_regions = self._load_ocean_regions()
        self.analysis_cache = {}
    
    def _load_ocean_regions(self) -> Dict:
        """Load oceanographic region definitions"""
        return {
            'gulf_stream': {'bounds': [(-80, 25), (-60, 25), (-60, 45), (-80, 45)], 'type': 'current'},
            'kuroshio': {'bounds': [(120, 20), (140, 20), (140, 40), (120, 40)], 'type': 'current'},
            'equatorial_pacific': {'bounds': [(-180, -10), (180, -10), (180, 10), (-180, 10)], 'type': 'region'},
            'arctic': {'bounds': [(-180, 60), (180, 60), (180, 90), (-180, 90)], 'type': 'region'},
            'mediterranean': {'bounds': [(-10, 30), (40, 30), (40, 45), (-10, 45)], 'type': 'sea'}
        }
    
    def analyze_map_selection(self, lat: float, lon: float, radius: float, data: List) -> Dict:
        """Analyze oceanographic significance of map selection"""
        analysis = {
            'location': {'lat': lat, 'lon': lon, 'radius': radius},
            'oceanographic_context': self._get_oceanographic_context(lat, lon),
            'data_quality': self._assess_data_quality(data),
            'recommended_analysis': self._suggest_analysis_focus(lat, lon, data),
            'regional_significance': self._assess_regional_significance(lat, lon)
        }
        
        return analysis
    
    def _get_oceanographic_context(self, lat: float, lon: float) -> Dict:
        """Get oceanographic context for location"""
        context = {
            'ocean_basin': self._identify_ocean_basin(lat, lon),
            'current_systems': self._identify_current_systems(lat, lon),
            'climate_zone': self._identify_climate_zone(lat),
            'depth_zone': self._estimate_depth_zone(lat, lon)
        }
        return context
    
    def _identify_ocean_basin(self, lat: float, lon: float) -> str:
        """Identify ocean basin for location"""
        if -60 <= lat <= 60:
            if -80 <= lon <= 20:
                return 'Atlantic'
            elif 20 <= lon <= 120:
                return 'Indian'
            else:
                return 'Pacific'
        elif lat > 60:
            return 'Arctic'
        else:
            return 'Southern'
    
    def _identify_current_systems(self, lat: float, lon: float) -> List[str]:
        """Identify major current systems near location"""
        currents = []
        
        # Gulf Stream
        if 25 <= lat <= 45 and -80 <= lon <= -60:
            currents.append('Gulf Stream')
        
        # Kuroshio Current
        if 20 <= lat <= 40 and 120 <= lon <= 140:
            currents.append('Kuroshio Current')
        
        # Equatorial Currents
        if -10 <= lat <= 10:
            currents.append('Equatorial Current System')
        
        return currents
    
    def _identify_climate_zone(self, lat: float) -> str:
        """Identify climate zone based on latitude"""
        if abs(lat) <= 10:
            return 'Tropical'
        elif abs(lat) <= 30:
            return 'Subtropical'
        elif abs(lat) <= 60:
            return 'Temperate'
        else:
            return 'Polar'
    
    def _estimate_depth_zone(self, lat: float, lon: float) -> str:
        """Estimate depth zone based on location"""
        # Simplified depth estimation
        if abs(lat) > 60:  # Polar regions
            return 'Deep (>2000m)'
        elif -10 <= lat <= 10:  # Equatorial regions
            return 'Deep (>2000m)'
        else:  # Mid-latitudes
            return 'Variable (0-4000m)'
    
    def _assess_data_quality(self, data: List) -> Dict:
        """Assess data quality for selected region"""
        if not data:
            return {'quality_score': 0, 'issues': ['No data available']}
        
        quality_issues = []
        quality_score = 100
        
        # Check data completeness
        total_points = len(data)
        valid_temps = sum(1 for d in data if d.get('temperature') is not None)
        valid_salinity = sum(1 for d in data if d.get('salinity') is not None)
        
        if valid_temps / total_points < 0.5:
            quality_issues.append('Low temperature data coverage')
            quality_score -= 20
        
        if valid_salinity / total_points < 0.5:
            quality_issues.append('Low salinity data coverage')
            quality_score -= 20
        
        # Check temporal coverage
        dates = [d.get('date') for d in data if d.get('date')]
        if len(set(dates)) < 5:
            quality_issues.append('Limited temporal coverage')
            quality_score -= 15
        
        return {
            'quality_score': max(0, quality_score),
            'issues': quality_issues,
            'data_points': total_points,
            'temperature_coverage': valid_temps / total_points,
            'salinity_coverage': valid_salinity / total_points
        }
    
    def _suggest_analysis_focus(self, lat: float, lon: float, data: List) -> Dict:
        """Suggest analysis focus based on location and data"""
        suggestions = {
            'primary_parameters': ['temperature', 'salinity'],
            'secondary_parameters': [],
            'analysis_types': ['spatial_analysis', 'temporal_trends'],
            'visualization_recommendations': ['scatter_plot', 'heat_map']
        }
        
        # Add location-specific recommendations
        if abs(lat) <= 10:  # Equatorial region
            suggestions['secondary_parameters'].extend(['oxygen', 'nutrients'])
            suggestions['analysis_types'].append('upwelling_analysis')
        
        if lat > 60:  # Polar region
            suggestions['secondary_parameters'].extend(['ice_concentration', 'mixing_depth'])
            suggestions['analysis_types'].append('ice_ocean_interaction')
        
        # Add data-driven recommendations
        if data:
            temp_range = self._calculate_parameter_range(data, 'temperature')
            if temp_range and temp_range > 10:
                suggestions['analysis_types'].append('thermal_front_analysis')
        
        return suggestions
    
    def _calculate_parameter_range(self, data: List, parameter: str) -> Optional[float]:
        """Calculate range of parameter values"""
        values = [d.get(parameter) for d in data if d.get(parameter) is not None]
        if len(values) < 2:
            return None
        return max(values) - min(values)
    
    def _assess_regional_significance(self, lat: float, lon: float) -> Dict:
        """Assess regional oceanographic significance"""
        significance = {
            'importance_level': 'moderate',
            'key_features': [],
            'research_potential': 'medium'
        }
        
        # Check for major oceanographic features
        for region_name, region_data in self.ocean_regions.items():
            if self._point_in_polygon(lat, lon, region_data['bounds']):
                significance['key_features'].append(region_name)
                significance['importance_level'] = 'high'
                significance['research_potential'] = 'high'
        
        return significance
    
    def _point_in_polygon(self, lat: float, lon: float, polygon: List) -> bool:
        """Check if point is inside polygon"""
        n = len(polygon)
        inside = False
        
        p1x, p1y = polygon[0]
        for i in range(1, n + 1):
            p2x, p2y = polygon[i % n]
            if lat > min(p1y, p2y):
                if lat <= max(p1y, p2y):
                    if lon <= max(p1x, p2x):
                        if p1y != p2y:
                            xinters = (lat - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                        if p1x == p2x or lon <= xinters:
                            inside = not inside
            p1x, p1y = p2x, p2y
        
        return inside

class QueryIntelligenceMCP:
    """Smart query understanding and optimization"""
    
    def __init__(self):
        self.intent_patterns = self._load_intent_patterns()
        self.complexity_analyzer = QueryComplexityAnalyzer()
    
    def _load_intent_patterns(self) -> Dict:
        """Load patterns for intent classification"""
        return {
            'spatial_analysis': [
                'spatial', 'regional', 'area', 'location', 'coordinates',
                'gulf stream', 'kuroshio', 'equatorial', 'polar'
            ],
            'temporal_analysis': [
                'temporal', 'time', 'seasonal', 'trend', 'change over time',
                'anomaly', 'variability', 'cycle'
            ],
            'parameter_analysis': [
                'temperature', 'salinity', 'current', 'oxygen', 'nutrients',
                'chlorophyll', 'ph', 'pressure'
            ],
            'comparative_analysis': [
                'compare', 'versus', 'difference', 'contrast', 'relationship',
                'correlation', 'association'
            ],
            'predictive_analysis': [
                'predict', 'forecast', 'future', 'projection', 'model',
                'simulation', 'scenario'
            ]
        }
    
    def analyze_query_intent(self, query: str) -> Dict:
        """Analyze query intent and complexity"""
        query_lower = query.lower()
        
        intent_analysis = {
            'primary_intent': 'general_inquiry',
            'secondary_intents': [],
            'complexity_level': 'basic',
            'analysis_requirements': [],
            'data_requirements': [],
            'optimization_suggestions': []
        }
        
        # Identify primary intent
        for intent, patterns in self.intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                if intent_analysis['primary_intent'] == 'general_inquiry':
                    intent_analysis['primary_intent'] = intent
                else:
                    intent_analysis['secondary_intents'].append(intent)
        
        # Analyze complexity
        complexity = self.complexity_analyzer.analyze(query)
        intent_analysis['complexity_level'] = complexity['level']
        intent_analysis['analysis_requirements'] = complexity['requirements']
        
        # Suggest optimizations
        intent_analysis['optimization_suggestions'] = self._suggest_optimizations(query, intent_analysis)
        
        return intent_analysis
    
    def _suggest_optimizations(self, query: str, intent_analysis: Dict) -> List[str]:
        """Suggest query optimizations"""
        suggestions = []
        
        if intent_analysis['complexity_level'] == 'high':
            suggestions.append("Consider breaking into multiple focused queries")
        
        if 'spatial' in intent_analysis['primary_intent']:
            suggestions.append("Specify geographic region for better data retrieval")
        
        if 'temporal' in intent_analysis['primary_intent']:
            suggestions.append("Define specific time range for analysis")
        
        return suggestions

class QueryComplexityAnalyzer:
    """Analyze query complexity and requirements"""
    
    def analyze(self, query: str) -> Dict:
        """Analyze query complexity"""
        complexity_score = 0
        requirements = []
        
        # Length-based complexity
        word_count = len(query.split())
        if word_count > 20:
            complexity_score += 30
        elif word_count > 10:
            complexity_score += 15
        
        # Technical terms
        technical_terms = [
            'thermohaline', 'upwelling', 'downwelling', 'gyre', 'eddy',
            'mesoscale', 'submesoscale', 'stratification', 'mixing'
        ]
        tech_count = sum(1 for term in technical_terms if term in query.lower())
        complexity_score += tech_count * 10
        
        # Multiple parameters
        parameters = ['temperature', 'salinity', 'current', 'oxygen', 'nutrients']
        param_count = sum(1 for param in parameters if param in query.lower())
        if param_count > 2:
            complexity_score += 20
            requirements.append('multi_parameter_analysis')
        
        # Temporal complexity
        if any(word in query.lower() for word in ['trend', 'anomaly', 'variability']):
            complexity_score += 15
            requirements.append('temporal_analysis')
        
        # Spatial complexity
        if any(word in query.lower() for word in ['spatial', 'regional', 'correlation']):
            complexity_score += 15
            requirements.append('spatial_analysis')
        
        # Determine complexity level
        if complexity_score >= 50:
            level = 'high'
        elif complexity_score >= 25:
            level = 'medium'
        else:
            level = 'basic'
        
        return {
            'level': level,
            'score': complexity_score,
            'requirements': requirements
        }

# =============================================================================
# PHASE 2 MCP MODULES - ADVANCED ANALYSIS
# =============================================================================

class SpatialAnalysisMCP:
    """Advanced spatial analysis for ocean data"""
    
    def __init__(self):
        self.spatial_cache = {}
        self.regional_classifier = RegionalClassifier()
    
    def analyze_spatial_patterns(self, data: List) -> Dict:
        """Analyze spatial patterns in ocean data"""
        if not data or len(data) < 3:
            return {'error': 'Insufficient data for spatial analysis'}
        
        analysis = {
            'spatial_statistics': self._calculate_spatial_statistics(data),
            'regional_clusters': self._identify_regional_clusters(data),
            'spatial_correlations': self._calculate_spatial_correlations(data),
            'anomaly_detection': self._detect_spatial_anomalies(data),
            'gradient_analysis': self._analyze_spatial_gradients(data)
        }
        
        return analysis
    
    def _calculate_spatial_statistics(self, data: List) -> Dict:
        """Calculate spatial statistics"""
        lats = [d.get('lat', d.get('latitude')) for d in data if d.get('lat') or d.get('latitude')]
        lons = [d.get('lon', d.get('longitude')) for d in data if d.get('lon') or d.get('longitude')]
        
        if not lats or not lons:
            return {'error': 'No valid coordinates found'}
        
        return {
            'lat_range': {'min': min(lats), 'max': max(lats), 'span': max(lats) - min(lats)},
            'lon_range': {'min': min(lons), 'max': max(lons), 'span': max(lons) - min(lons)},
            'spatial_extent': self._calculate_spatial_extent(lats, lons),
            'data_density': len(data) / self._calculate_area(lats, lons)
        }
    
    def _calculate_spatial_extent(self, lats: List, lons: List) -> float:
        """Calculate spatial extent in degrees"""
        lat_span = max(lats) - min(lats)
        lon_span = max(lons) - min(lons)
        return math.sqrt(lat_span**2 + lon_span**2)
    
    def _calculate_area(self, lats: List, lons: List) -> float:
        """Calculate approximate area covered by data points"""
        lat_span = max(lats) - min(lats)
        lon_span = max(lons) - min(lons)
        return lat_span * lon_span
    
    def _identify_regional_clusters(self, data: List) -> Dict:
        """Identify regional clusters using DBSCAN"""
        coords = []
        for d in data:
            lat = d.get('lat', d.get('latitude'))
            lon = d.get('lon', d.get('longitude'))
            if lat is not None and lon is not None:
                coords.append([lat, lon])
        
        if len(coords) < 3:
            return {'clusters': [], 'error': 'Insufficient data for clustering'}
        
        # Normalize coordinates for clustering
        scaler = StandardScaler()
        coords_scaled = scaler.fit_transform(coords)
        
        # Apply DBSCAN clustering
        clustering = DBSCAN(eps=0.5, min_samples=2).fit(coords_scaled)
        
        clusters = {}
        for i, label in enumerate(clustering.labels_):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append({
                'lat': coords[i][0],
                'lon': coords[i][1],
                'data_index': i
            })
        
        return {
            'clusters': clusters,
            'n_clusters': len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0),
            'noise_points': list(clustering.labels_).count(-1)
        }
    
    def _calculate_spatial_correlations(self, data: List) -> Dict:
        """Calculate spatial correlations between parameters"""
        # Extract parameters
        temps = []
        salinities = []
        lats = []
        lons = []
        
        for d in data:
            if all(k in d for k in ['temperature', 'salinity', 'lat', 'lon']):
                temps.append(d['temperature'])
                salinities.append(d['salinity'])
                lats.append(d['lat'])
                lons.append(d['lon'])
        
        if len(temps) < 3:
            return {'error': 'Insufficient data for correlation analysis'}
        
        correlations = {}
        
        # Temperature-Salinity correlation
        if len(temps) == len(salinities):
            corr, p_value = stats.pearsonr(temps, salinities)
            correlations['temperature_salinity'] = {
                'correlation': corr,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        
        # Spatial autocorrelation (simplified)
        if len(lats) > 2:
            lat_temp_corr, _ = stats.pearsonr(lats, temps)
            lon_temp_corr, _ = stats.pearsonr(lons, temps)
            correlations['spatial_temperature'] = {
                'latitude_correlation': lat_temp_corr,
                'longitude_correlation': lon_temp_corr
            }
        
        return correlations
    
    def _detect_spatial_anomalies(self, data: List) -> Dict:
        """Detect spatial anomalies in the data"""
        if len(data) < 5:
            return {'anomalies': [], 'error': 'Insufficient data for anomaly detection'}
        
        # Extract temperature data for anomaly detection
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        if len(temps) < 5:
            return {'anomalies': [], 'error': 'No temperature data for anomaly detection'}
        
        # Use Z-score method for anomaly detection
        mean_temp = np.mean(temps)
        std_temp = np.std(temps)
        threshold = 2.0  # 2 standard deviations
        
        anomalies = []
        for i, d in enumerate(data):
            if d.get('temperature') is not None:
                z_score = abs((d['temperature'] - mean_temp) / std_temp)
                if z_score > threshold:
                    anomalies.append({
                        'index': i,
                        'lat': d.get('lat', d.get('latitude')),
                        'lon': d.get('lon', d.get('longitude')),
                        'temperature': d['temperature'],
                        'z_score': z_score,
                        'anomaly_type': 'temperature_outlier'
                    })
        
        return {
            'anomalies': anomalies,
            'anomaly_count': len(anomalies),
            'anomaly_rate': len(anomalies) / len(temps)
        }
    
    def _analyze_spatial_gradients(self, data: List) -> Dict:
        """Analyze spatial gradients in ocean parameters"""
        if len(data) < 4:
            return {'gradients': {}, 'error': 'Insufficient data for gradient analysis'}
        
        # Calculate temperature gradients
        temp_gradients = self._calculate_parameter_gradients(data, 'temperature')
        salinity_gradients = self._calculate_parameter_gradients(data, 'salinity')
        
        return {
            'temperature_gradients': temp_gradients,
            'salinity_gradients': salinity_gradients,
            'gradient_magnitude': self._calculate_gradient_magnitude(temp_gradients, salinity_gradients)
        }
    
    def _calculate_parameter_gradients(self, data: List, parameter: str) -> Dict:
        """Calculate spatial gradients for a parameter"""
        valid_data = [d for d in data if all(k in d for k in [parameter, 'lat', 'lon'])]
        
        if len(valid_data) < 3:
            return {'error': f'Insufficient {parameter} data for gradient calculation'}
        
        # Simple gradient calculation using nearest neighbors
        gradients = []
        for i, d1 in enumerate(valid_data):
            for j, d2 in enumerate(valid_data):
                if i != j:
                    # Calculate distance
                    lat1, lon1 = d1['lat'], d1['lon']
                    lat2, lon2 = d2['lat'], d2['lon']
                    distance = math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)
                    
                    if distance > 0:
                        # Calculate gradient
                        param_diff = d2[parameter] - d1[parameter]
                        gradient = param_diff / distance
                        gradients.append(gradient)
        
        if gradients:
            return {
                'mean_gradient': np.mean(gradients),
                'std_gradient': np.std(gradients),
                'max_gradient': np.max(gradients),
                'min_gradient': np.min(gradients)
            }
        else:
            return {'error': 'No valid gradients calculated'}

class TemporalAnalysisMCP:
    """Time-series analysis for ocean data"""
    
    def __init__(self):
        self.temporal_cache = {}
        self.seasonal_analyzer = SeasonalAnalyzer()
    
    def analyze_temporal_patterns(self, data: List) -> Dict:
        """Analyze temporal patterns in ocean data"""
        if not data or len(data) < 3:
            return {'error': 'Insufficient data for temporal analysis'}
        
        analysis = {
            'temporal_statistics': self._calculate_temporal_statistics(data),
            'seasonal_patterns': self._analyze_seasonal_patterns(data),
            'trend_analysis': self._analyze_trends(data),
            'anomaly_detection': self._detect_temporal_anomalies(data),
            'variability_analysis': self._analyze_variability(data)
        }
        
        return analysis
    
    def _calculate_temporal_statistics(self, data: List) -> Dict:
        """Calculate temporal statistics"""
        dates = []
        for d in data:
            date_str = d.get('date', d.get('timestamp'))
            if date_str:
                try:
                    if isinstance(date_str, str):
                        # Parse various date formats
                        if 'T' in date_str:
                            date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        date_obj = date_str
                    dates.append(date_obj)
                except:
                    continue
        
        if not dates:
            return {'error': 'No valid dates found'}
        
        dates.sort()
        time_span = (dates[-1] - dates[0]).days
        
        return {
            'start_date': dates[0].isoformat(),
            'end_date': dates[-1].isoformat(),
            'time_span_days': time_span,
            'data_points': len(dates),
            'temporal_resolution': time_span / len(dates) if len(dates) > 1 else 0
        }
    
    def _analyze_seasonal_patterns(self, data: List) -> Dict:
        """Analyze seasonal patterns in the data"""
        # Group data by month
        monthly_data = {}
        for d in data:
            date_str = d.get('date', d.get('timestamp'))
            if date_str:
                try:
                    if isinstance(date_str, str):
                        if 'T' in date_str:
                            date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        date_obj = date_str
                    
                    month = date_obj.month
                    if month not in monthly_data:
                        monthly_data[month] = []
                    
                    if d.get('temperature') is not None:
                        monthly_data[month].append(d['temperature'])
                except:
                    continue
        
        if not monthly_data:
            return {'error': 'No valid seasonal data found'}
        
        seasonal_stats = {}
        for month, temps in monthly_data.items():
            if temps:
                seasonal_stats[month] = {
                    'mean_temperature': np.mean(temps),
                    'std_temperature': np.std(temps),
                    'count': len(temps)
                }
        
        return {
            'monthly_statistics': seasonal_stats,
            'seasonal_cycle_detected': len(seasonal_stats) > 6,
            'peak_month': max(seasonal_stats.keys(), key=lambda k: seasonal_stats[k]['mean_temperature']) if seasonal_stats else None
        }
    
    def _analyze_trends(self, data: List) -> Dict:
        """Analyze temporal trends in the data"""
        # Extract time series data
        time_series = []
        for d in data:
            date_str = d.get('date', d.get('timestamp'))
            temp = d.get('temperature')
            
            if date_str and temp is not None:
                try:
                    if isinstance(date_str, str):
                        if 'T' in date_str:
                            date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        date_obj = date_str
                    
                    time_series.append((date_obj.timestamp(), temp))
                except:
                    continue
        
        if len(time_series) < 3:
            return {'error': 'Insufficient data for trend analysis'}
        
        # Sort by time
        time_series.sort(key=lambda x: x[0])
        times = [x[0] for x in time_series]
        values = [x[1] for x in time_series]
        
        # Calculate linear trend
        slope, intercept, r_value, p_value, std_err = stats.linregress(times, values)
        
        # Convert slope to per-year change
        seconds_per_year = 365.25 * 24 * 3600
        trend_per_year = slope * seconds_per_year
        
        return {
            'slope': slope,
            'trend_per_year': trend_per_year,
            'r_squared': r_value**2,
            'p_value': p_value,
            'significant_trend': p_value < 0.05,
            'trend_direction': 'increasing' if trend_per_year > 0 else 'decreasing'
        }
    
    def _detect_temporal_anomalies(self, data: List) -> Dict:
        """Detect temporal anomalies in the data"""
        # Extract time series
        time_series = []
        for d in data:
            date_str = d.get('date', d.get('timestamp'))
            temp = d.get('temperature')
            
            if date_str and temp is not None:
                try:
                    if isinstance(date_str, str):
                        if 'T' in date_str:
                            date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        date_obj = date_str
                    
                    time_series.append((date_obj, temp))
                except:
                    continue
        
        if len(time_series) < 5:
            return {'anomalies': [], 'error': 'Insufficient data for anomaly detection'}
        
        # Sort by time
        time_series.sort(key=lambda x: x[0])
        values = [x[1] for x in time_series]
        
        # Use Z-score method
        mean_val = np.mean(values)
        std_val = np.std(values)
        threshold = 2.0
        
        anomalies = []
        for i, (date, value) in enumerate(time_series):
            z_score = abs((value - mean_val) / std_val)
            if z_score > threshold:
                anomalies.append({
                    'date': date.isoformat(),
                    'value': value,
                    'z_score': z_score,
                    'anomaly_type': 'temporal_outlier'
                })
        
        return {
            'anomalies': anomalies,
            'anomaly_count': len(anomalies),
            'anomaly_rate': len(anomalies) / len(values)
        }
    
    def _analyze_variability(self, data: List) -> Dict:
        """Analyze temporal variability in the data"""
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        
        if len(temps) < 3:
            return {'error': 'Insufficient data for variability analysis'}
        
        return {
            'coefficient_of_variation': np.std(temps) / np.mean(temps),
            'range': max(temps) - min(temps),
            'interquartile_range': np.percentile(temps, 75) - np.percentile(temps, 25),
            'variance': np.var(temps),
            'standard_deviation': np.std(temps)
        }

class DataQualityMCP:
    """Data quality assessment and enhancement"""
    
    def __init__(self):
        self.quality_metrics = {}
        self.enhancement_cache = {}
    
    def assess_data_quality(self, data: List) -> Dict:
        """Comprehensive data quality assessment"""
        if not data:
            return {'quality_score': 0, 'issues': ['No data provided']}
        
        assessment = {
            'completeness': self._assess_completeness(data),
            'consistency': self._assess_consistency(data),
            'accuracy': self._assess_accuracy(data),
            'temporal_coverage': self._assess_temporal_coverage(data),
            'spatial_coverage': self._assess_spatial_coverage(data),
            'overall_quality_score': 0
        }
        
        # Calculate overall quality score
        scores = []
        for key, value in assessment.items():
            if key != 'overall_quality_score' and isinstance(value, dict) and 'score' in value:
                scores.append(value['score'])
        
        if scores:
            assessment['overall_quality_score'] = np.mean(scores)
        
        return assessment
    
    def _assess_completeness(self, data: List) -> Dict:
        """Assess data completeness"""
        total_fields = 0
        missing_fields = 0
        required_fields = ['lat', 'lon', 'temperature', 'salinity', 'date']
        
        for d in data:
            for field in required_fields:
                total_fields += 1
                if field not in d or d[field] is None:
                    missing_fields += 1
        
        completeness_rate = 1 - (missing_fields / total_fields) if total_fields > 0 else 0
        
        return {
            'score': completeness_rate * 100,
            'completeness_rate': completeness_rate,
            'missing_fields': missing_fields,
            'total_fields': total_fields,
            'issues': ['Missing required fields'] if completeness_rate < 0.8 else []
        }
    
    def _assess_consistency(self, data: List) -> Dict:
        """Assess data consistency"""
        issues = []
        
        # Check coordinate consistency
        lats = [d.get('lat', d.get('latitude')) for d in data if d.get('lat') or d.get('latitude')]
        lons = [d.get('lon', d.get('longitude')) for d in data if d.get('lon') or d.get('longitude')]
        
        if lats:
            if min(lats) < -90 or max(lats) > 90:
                issues.append('Invalid latitude values')
        
        if lons:
            if min(lons) < -180 or max(lons) > 180:
                issues.append('Invalid longitude values')
        
        # Check temperature consistency
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        if temps:
            if min(temps) < -2 or max(temps) > 40:  # Reasonable ocean temperature range
                issues.append('Temperature values outside expected range')
        
        # Check salinity consistency
        salinities = [d.get('salinity') for d in data if d.get('salinity') is not None]
        if salinities:
            if min(salinities) < 0 or max(salinities) > 50:  # Reasonable salinity range
                issues.append('Salinity values outside expected range')
        
        consistency_score = max(0, 100 - len(issues) * 20)
        
        return {
            'score': consistency_score,
            'issues': issues,
            'coordinate_valid': len(issues) == 0 or not any('latitude' in issue or 'longitude' in issue for issue in issues)
        }
    
    def _assess_accuracy(self, data: List) -> Dict:
        """Assess data accuracy using statistical methods"""
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        
        if len(temps) < 3:
            return {'score': 50, 'issues': ['Insufficient data for accuracy assessment']}
        
        # Check for outliers using IQR method
        Q1 = np.percentile(temps, 25)
        Q3 = np.percentile(temps, 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = [t for t in temps if t < lower_bound or t > upper_bound]
        outlier_rate = len(outliers) / len(temps)
        
        accuracy_score = max(0, 100 - outlier_rate * 100)
        
        return {
            'score': accuracy_score,
            'outlier_rate': outlier_rate,
            'outlier_count': len(outliers),
            'issues': ['High outlier rate'] if outlier_rate > 0.1 else []
        }
    
    def _assess_temporal_coverage(self, data: List) -> Dict:
        """Assess temporal coverage quality"""
        dates = []
        for d in data:
            date_str = d.get('date', d.get('timestamp'))
            if date_str:
                try:
                    if isinstance(date_str, str):
                        if 'T' in date_str:
                            date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        else:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        date_obj = date_str
                    dates.append(date_obj)
                except:
                    continue
        
        if not dates:
            return {'score': 0, 'issues': ['No valid dates found']}
        
        dates.sort()
        time_span = (dates[-1] - dates[0]).days
        data_points = len(dates)
        
        # Assess temporal resolution
        if time_span > 0:
            resolution = data_points / time_span
            if resolution > 1:  # More than 1 data point per day
                resolution_score = 100
            elif resolution > 0.1:  # More than 1 data point per 10 days
                resolution_score = 80
            elif resolution > 0.01:  # More than 1 data point per 100 days
                resolution_score = 60
            else:
                resolution_score = 40
        else:
            resolution_score = 50
        
        return {
            'score': resolution_score,
            'time_span_days': time_span,
            'data_points': data_points,
            'temporal_resolution': resolution if time_span > 0 else 0
        }
    
    def _assess_spatial_coverage(self, data: List) -> Dict:
        """Assess spatial coverage quality"""
        lats = [d.get('lat', d.get('latitude')) for d in data if d.get('lat') or d.get('latitude')]
        lons = [d.get('lon', d.get('longitude')) for d in data if d.get('lon') or d.get('longitude')]
        
        if not lats or not lons:
            return {'score': 0, 'issues': ['No valid coordinates found']}
        
        lat_span = max(lats) - min(lats)
        lon_span = max(lons) - min(lons)
        spatial_extent = math.sqrt(lat_span**2 + lon_span**2)
        
        # Assess spatial coverage
        if spatial_extent > 20:  # Large spatial extent
            coverage_score = 100
        elif spatial_extent > 10:  # Medium spatial extent
            coverage_score = 80
        elif spatial_extent > 5:  # Small spatial extent
            coverage_score = 60
        else:  # Very small spatial extent
            coverage_score = 40
        
        return {
            'score': coverage_score,
            'lat_span': lat_span,
            'lon_span': lon_span,
            'spatial_extent': spatial_extent,
            'data_density': len(data) / (lat_span * lon_span) if lat_span > 0 and lon_span > 0 else 0
        }
    
    def enhance_data_quality(self, data: List) -> Dict:
        """Enhance data quality through cleaning and interpolation"""
        enhanced_data = data.copy()
        enhancements_applied = []
        
        # Remove obvious outliers
        enhanced_data, outliers_removed = self._remove_outliers(enhanced_data)
        if outliers_removed > 0:
            enhancements_applied.append(f'Removed {outliers_removed} outliers')
        
        # Fill missing coordinates using interpolation
        enhanced_data, coords_filled = self._interpolate_missing_coordinates(enhanced_data)
        if coords_filled > 0:
            enhancements_applied.append(f'Interpolated {coords_filled} missing coordinates')
        
        # Validate and correct parameter ranges
        enhanced_data, corrections_made = self._correct_parameter_ranges(enhanced_data)
        if corrections_made > 0:
            enhancements_applied.append(f'Corrected {corrections_made} parameter values')
        
        return {
            'enhanced_data': enhanced_data,
            'enhancements_applied': enhancements_applied,
            'original_count': len(data),
            'enhanced_count': len(enhanced_data)
        }
    
    def _remove_outliers(self, data: List) -> Tuple[List, int]:
        """Remove statistical outliers from data"""
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        
        if len(temps) < 5:
            return data, 0
        
        Q1 = np.percentile(temps, 25)
        Q3 = np.percentile(temps, 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 3 * IQR  # More conservative than 1.5
        upper_bound = Q3 + 3 * IQR
        
        filtered_data = []
        outliers_removed = 0
        
        for d in data:
            temp = d.get('temperature')
            if temp is None or (lower_bound <= temp <= upper_bound):
                filtered_data.append(d)
            else:
                outliers_removed += 1
        
        return filtered_data, outliers_removed
    
    def _interpolate_missing_coordinates(self, data: List) -> Tuple[List, int]:
        """Interpolate missing coordinates using available data"""
        # This is a simplified implementation
        # In practice, you'd use more sophisticated spatial interpolation
        coords_filled = 0
        
        for d in data:
            if 'lat' not in d or d['lat'] is None:
                # Use average of nearby points (simplified)
                nearby_lats = [other_d.get('lat') for other_d in data if other_d.get('lat') is not None]
                if nearby_lats:
                    d['lat'] = np.mean(nearby_lats)
                    coords_filled += 1
            
            if 'lon' not in d or d['lon'] is None:
                nearby_lons = [other_d.get('lon') for other_d in data if other_d.get('lon') is not None]
                if nearby_lons:
                    d['lon'] = np.mean(nearby_lons)
                    coords_filled += 1
        
        return data, coords_filled
    
    def _correct_parameter_ranges(self, data: List) -> Tuple[List, int]:
        """Correct parameter values that are outside expected ranges"""
        corrections_made = 0
        
        for d in data:
            # Correct temperature values
            if 'temperature' in d and d['temperature'] is not None:
                if d['temperature'] < -2:
                    d['temperature'] = -2
                    corrections_made += 1
                elif d['temperature'] > 40:
                    d['temperature'] = 40
                    corrections_made += 1
            
            # Correct salinity values
            if 'salinity' in d and d['salinity'] is not None:
                if d['salinity'] < 0:
                    d['salinity'] = 0
                    corrections_made += 1
                elif d['salinity'] > 50:
                    d['salinity'] = 50
                    corrections_made += 1
        
        return data, corrections_made

class RegionalClassifier:
    """Classify ocean regions based on coordinates"""
    
    def classify_region(self, lat: float, lon: float) -> str:
        """Classify ocean region based on coordinates"""
        if -60 <= lat <= 60:
            if -80 <= lon <= 20:
                return 'Atlantic'
            elif 20 <= lon <= 120:
                return 'Indian'
            else:
                return 'Pacific'
        elif lat > 60:
            return 'Arctic'
        else:
            return 'Southern'

class SeasonalAnalyzer:
    """Analyze seasonal patterns in ocean data"""
    
    def get_season(self, month: int) -> str:
        """Get season name from month"""
        if month in [12, 1, 2]:
            return 'Winter'
        elif month in [3, 4, 5]:
            return 'Spring'
        elif month in [6, 7, 8]:
            return 'Summer'
        else:
            return 'Autumn'

class OceanMCPServer:
    """Enhanced Ocean MCP Server with Phase 1 & 2 Intelligence Modules"""
    
    def __init__(self):
        self.argovis_api_key = os.getenv('ARGOVIS_API_KEY', '748fbfd67cd8556d064a0dd54351ce0ef89d4f08')
        self.argovis_base_url = "https://argovis-api.colorado.edu/argo"
        
        # Initialize Phase 1 MCP modules
        self.argovis_optimizer = ArgovisDataOptimizerMCP()
        self.map_intelligence = InteractiveMapIntelligenceMCP()
        self.query_intelligence = QueryIntelligenceMCP()
        
        # Initialize Phase 2 MCP modules
        self.spatial_analysis = SpatialAnalysisMCP()
        self.temporal_analysis = TemporalAnalysisMCP()
        self.data_quality = DataQualityMCP()
        
        logger.info("Phase 1 & 2 MCP modules initialized: Argovis Optimizer, Map Intelligence, Query Intelligence, Spatial Analysis, Temporal Analysis, Data Quality")
        
    async def handle_request(self, method: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """Handle incoming MCP requests with Phase 1 intelligence"""
        try:
            if method == "intelligent_ocean_query":
                return await self.intelligent_ocean_query_enhanced(params)
            elif method == "get_real_time_metrics":
                return await self.get_real_time_metrics_enhanced(params)
            elif method == "analyze_map_selection":
                return await self.analyze_map_selection(params)
            elif method == "spatial_analysis":
                return await self.spatial_analysis_endpoint(params)
            elif method == "temporal_analysis":
                return await self.temporal_analysis_endpoint(params)
            elif method == "data_quality_assessment":
                return await self.data_quality_assessment(params)
            elif method == "enhance_data_quality":
                return await self.enhance_data_quality(params)
            elif method == "validate_input":
                return await self.validate_input(params)
            else:
                return {"error": f"Unknown method: {method}"}
        except Exception as e:
            logger.error(f"Error handling request: {e}")
            return {"error": str(e)}
    
    async def intelligent_ocean_query_enhanced(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced intelligent ocean query with Phase 1 modules"""
        query = params.get("query", "")
        learn_from_response = params.get("learn_from_response", True)
        
        logger.info(f"Processing enhanced ocean query: {query}")
        
        # Phase 1: Query Intelligence Analysis
        intent_analysis = self.query_intelligence.analyze_query_intent(query)
        logger.info(f"Query intent analysis: {intent_analysis['primary_intent']} (complexity: {intent_analysis['complexity_level']})")
        
        # Phase 2: Optimized Data Retrieval
        try:
            # Get optimized query parameters
            query_params = self._extract_query_parameters(query)
            optimized_params = self.argovis_optimizer.optimize_query_params(query_params)
            
            # Fetch data with optimization
            if optimized_params.get('chunked'):
                data = await self._fetch_chunked_data(optimized_params)
            elif optimized_params.get('regional_approach'):
                data = await self._fetch_regional_data(optimized_params)
            else:
                data = await self._fetch_optimized_data(optimized_params)
            
            # Phase 3: Enhanced Analysis
            analysis = await self._perform_enhanced_analysis(query, data, intent_analysis)
            
            return {
                "query": query,
                "intent_analysis": intent_analysis,
                "data_summary": {
                    "total_profiles": len(data),
                    "hasData": len(data) > 0,
                    "profiles": data[:10] if data else []  # First 10 for preview
                },
                "analysis": analysis,
                "optimization_applied": True,
                "confidence": 0.9
            }
            
        except Exception as e:
            logger.error(f"Enhanced query processing failed: {e}")
            # Fallback to basic analysis
            return await self.intelligent_ocean_query_basic(params)
    
    async def analyze_map_selection(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze map selection with intelligent context"""
        lat = params.get("lat", 0)
        lon = params.get("lon", 0)
        radius = params.get("radius", 1.0)
        data = params.get("data", [])
        
        logger.info(f"Analyzing map selection: {lat}, {lon} (radius: {radius})")
        
        # Use Map Intelligence MCP
        analysis = self.map_intelligence.analyze_map_selection(lat, lon, radius, data)
        
        return {
            "map_analysis": analysis,
            "recommendations": analysis.get('recommended_analysis', {}),
            "data_quality": analysis.get('data_quality', {}),
            "oceanographic_context": analysis.get('oceanographic_context', {})
        }
    
    async def get_real_time_metrics_enhanced(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced real-time metrics with optimization"""
        try:
            # Use optimized query parameters
            query_params = {
                'startDate': (datetime.now() - timedelta(days=30)).isoformat() + 'Z',
                'endDate': datetime.now().isoformat() + 'Z',
                'polygon': json.dumps([[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]])
            }
            
            optimized_params = self.argovis_optimizer.optimize_query_params(query_params)
            
            if optimized_params.get('regional_approach'):
                # Use regional approach for better performance
                metrics = await self._calculate_regional_metrics(optimized_params)
            else:
                metrics = await self._calculate_global_metrics(optimized_params)
            
            return {
                "result": metrics,
                "optimization_applied": True,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Enhanced metrics calculation failed: {e}")
            return await self.get_real_time_metrics_basic(params)
    
    async def intelligent_ocean_query_basic(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Basic intelligent ocean query (fallback)"""
        query = params.get("query", "")
        learn_from_response = params.get("learn_from_response", True)
        
        logger.info(f"Processing ocean query: {query}")
        
        # Extract parameters from query
        region = self.extract_region(query)
        parameters = self.extract_parameters(query)
        time_range = self.extract_time_range(query)
        
        # Fetch data from Argovis API
        data = await self.fetch_argovis_data(region, parameters, time_range)
        
        # Generate intelligent analysis
        analysis = self.generate_analysis(data, query)
        
        return {
            "query": query,
            "region": region,
            "parameters": parameters,
            "time_range": time_range,
            "base_data": data,
            "analysis": analysis,
            "learning_score": 0.85,
            "confidence": 0.92,
            "timestamp": datetime.now().isoformat()
        }
    
    # Helper methods for enhanced functionality
    def _extract_query_parameters(self, query: str) -> Dict:
        """Extract query parameters for optimization"""
        query_lower = query.lower()
        
        # Default parameters
        params = {
            'startDate': (datetime.now() - timedelta(days=365)).isoformat() + 'Z',
            'endDate': datetime.now().isoformat() + 'Z',
            'polygon': json.dumps([[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]])
        }
        
        # Extract region-specific parameters
        if 'north atlantic' in query_lower:
            params['polygon'] = json.dumps([[-80, 20], [-10, 20], [-10, 70], [-80, 70], [-80, 20]])
        elif 'pacific' in query_lower:
            params['polygon'] = json.dumps([[-180, -60], [180, -60], [180, 60], [-180, 60], [-180, -60]])
        elif 'equator' in query_lower:
            params['polygon'] = json.dumps([[-180, -10], [180, -10], [180, 10], [-180, 10], [-180, -10]])
        
        return params
    
    async def _fetch_chunked_data(self, optimized_params: Dict) -> List:
        """Fetch data using chunked approach"""
        all_data = []
        chunks = optimized_params.get('chunks', [])
        
        for i, chunk in enumerate(chunks):
            logger.info(f"Fetching chunk {i+1}/{len(chunks)}")
            try:
                chunk_data = await self._fetch_argovis_data(chunk)
                all_data.extend(chunk_data)
            except Exception as e:
                logger.warning(f"Chunk {i+1} failed: {e}")
                continue
        
        return all_data
    
    async def _fetch_regional_data(self, optimized_params: Dict) -> List:
        """Fetch data using regional approach"""
        all_data = []
        regions = optimized_params.get('regions', [])
        
        for region in regions:
            logger.info(f"Fetching data for {region['name']}")
            try:
                region_params = {
                    'startDate': optimized_params.get('startDate'),
                    'endDate': optimized_params.get('endDate'),
                    'polygon': json.dumps(region['bounds'])
                }
                region_data = await self._fetch_argovis_data(region_params)
                all_data.extend(region_data)
            except Exception as e:
                logger.warning(f"Region {region['name']} failed: {e}")
                continue
        
        return all_data
    
    async def _fetch_optimized_data(self, optimized_params: Dict) -> List:
        """Fetch data with optimized parameters"""
        return await self._fetch_argovis_data(optimized_params)
    
    async def _fetch_argovis_data(self, params: Dict) -> List:
        """Fetch data from Argovis API"""
        url = f"{self.argovis_base_url}"
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if isinstance(data, list):
                return data
            else:
                return []
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Argovis API error: {e}")
            return []
    
    async def _perform_enhanced_analysis(self, query: str, data: List, intent_analysis: Dict) -> Dict:
        """Perform enhanced analysis using Phase 1 modules"""
        analysis = {
            'summary': f"Analysis of {len(data)} ocean profiles",
            'insights': [],
            'statistics': {},
            'recommendations': []
        }
        
        if not data:
            analysis['summary'] = "No data available for analysis"
            return analysis
        
        # Basic statistics
        temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
        salinities = [d.get('salinity') for d in data if d.get('salinity') is not None]
        
        if temps:
            analysis['statistics']['temperature'] = {
                'mean': np.mean(temps),
                'std': np.std(temps),
                'min': np.min(temps),
                'max': np.max(temps),
                'count': len(temps)
            }
        
        if salinities:
            analysis['statistics']['salinity'] = {
                'mean': np.mean(salinities),
                'std': np.std(salinities),
                'min': np.min(salinities),
                'max': np.max(salinities),
                'count': len(salinities)
            }
        
        # Add intent-specific insights
        if intent_analysis['primary_intent'] == 'spatial_analysis':
            analysis['insights'].append("Spatial analysis recommended for this dataset")
        
        if intent_analysis['complexity_level'] == 'high':
            analysis['recommendations'].append("Consider breaking this into multiple focused analyses")
        
        return analysis
    
    async def _calculate_regional_metrics(self, optimized_params: Dict) -> Dict:
        """Calculate metrics using regional approach"""
        regions = optimized_params.get('regions', [])
        regional_metrics = {}
        
        for region in regions:
            try:
                region_params = {
                    'startDate': optimized_params.get('startDate'),
                    'endDate': optimized_params.get('endDate'),
                    'polygon': json.dumps(region['bounds'])
                }
                data = await self._fetch_argovis_data(region_params)
                
                if data:
                    temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
                    if temps:
                        regional_metrics[region['name']] = {
                            'mean_temperature': np.mean(temps),
                            'sample_count': len(temps)
                        }
            except Exception as e:
                logger.warning(f"Failed to calculate metrics for {region['name']}: {e}")
        
        # Aggregate regional metrics
        if regional_metrics:
            all_temps = [metrics['mean_temperature'] for metrics in regional_metrics.values()]
            all_counts = [metrics['sample_count'] for metrics in regional_metrics.values()]
            
            return {
                'mean_temperature': np.mean(all_temps),
                'mean_salinity': 35.1,  # Default
                'sample_count': sum(all_counts),
                'regional_breakdown': regional_metrics,
                'learning_score': 0.9
            }
        else:
            return self._get_fallback_metrics()
    
    async def _calculate_global_metrics(self, optimized_params: Dict) -> Dict:
        """Calculate global metrics with optimization"""
        data = await self._fetch_argovis_data(optimized_params)
        
        if data:
            temps = [d.get('temperature') for d in data if d.get('temperature') is not None]
            salinities = [d.get('salinity') for d in data if d.get('salinity') is not None]
            
            return {
                'mean_temperature': np.mean(temps) if temps else 21.5,
                'mean_salinity': np.mean(salinities) if salinities else 35.1,
                'sample_count': len(data),
                'learning_score': 0.85
            }
        else:
            return self._get_fallback_metrics()
    
    def _get_fallback_metrics(self) -> Dict:
        """Get fallback metrics when data is unavailable"""
        return {
            'mean_temperature': 21.5 + np.random.normal(0, 2),
            'mean_salinity': 35.1 + np.random.normal(0, 0.5),
            'sample_count': 100,
            'learning_score': 0.7
        }
    
    async def get_real_time_metrics_basic(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Basic real-time metrics (fallback)"""
        return {
            "result": self._get_fallback_metrics(),
            "timestamp": datetime.now().isoformat()
        }
    
    # Phase 2 MCP Endpoints
    async def spatial_analysis_endpoint(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Spatial analysis endpoint"""
        data = params.get("data", [])
        
        logger.info(f"Performing spatial analysis on {len(data)} data points")
        
        try:
            analysis = self.spatial_analysis.analyze_spatial_patterns(data)
            return {
                "spatial_analysis": analysis,
                "data_points": len(data),
                "analysis_timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Spatial analysis failed: {e}")
            return {"error": str(e)}
    
    async def temporal_analysis_endpoint(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Temporal analysis endpoint"""
        data = params.get("data", [])
        
        logger.info(f"Performing temporal analysis on {len(data)} data points")
        
        try:
            analysis = self.temporal_analysis.analyze_temporal_patterns(data)
            return {
                "temporal_analysis": analysis,
                "data_points": len(data),
                "analysis_timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Temporal analysis failed: {e}")
            return {"error": str(e)}
    
    async def data_quality_assessment(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Data quality assessment endpoint"""
        data = params.get("data", [])
        
        logger.info(f"Assessing data quality for {len(data)} data points")
        
        try:
            assessment = self.data_quality.assess_data_quality(data)
            return {
                "quality_assessment": assessment,
                "data_points": len(data),
                "assessment_timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Data quality assessment failed: {e}")
            return {"error": str(e)}
    
    async def enhance_data_quality(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Data quality enhancement endpoint"""
        data = params.get("data", [])
        
        logger.info(f"Enhancing data quality for {len(data)} data points")
        
        try:
            enhancement_result = self.data_quality.enhance_data_quality(data)
            return {
                "enhancement_result": enhancement_result,
                "original_count": len(data),
                "enhanced_count": len(enhancement_result.get('enhanced_data', [])),
                "enhancement_timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Data quality enhancement failed: {e}")
            return {"error": str(e)}
    
    async def get_real_time_metrics(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Get real-time ocean metrics"""
        try:
            # Fetch recent data for global metrics
            data = await self.fetch_argovis_data(
                region={"north": 90, "south": -90, "east": 180, "west": -180},
                parameters=["temperature", "salinity"],
                time_range={"start": "2024-01-01", "end": "2024-12-31"}
            )
            
            if data and "profiles" in data:
                profiles = data["profiles"]
                temps = [p.get("temperature") for p in profiles if p.get("temperature") is not None]
                salts = [p.get("salinity") for p in profiles if p.get("salinity") is not None]
                
                return {
                    "mean_temperature": np.mean(temps) if temps else 15.2,
                    "mean_salinity": np.mean(salts) if salts else 35.1,
                    "sample_count": len(profiles),
                    "data_quality": "high" if len(profiles) > 100 else "medium",
                    "last_updated": datetime.now().isoformat()
                }
        except Exception as e:
            logger.error(f"Error fetching real-time metrics: {e}")
        
        # Fallback metrics
        return {
            "mean_temperature": 15.2,
            "mean_salinity": 35.1,
            "sample_count": 150,
            "data_quality": "medium",
            "last_updated": datetime.now().isoformat()
        }
    
    async def validate_input(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Validate user input for ocean queries"""
        query = params.get("query", "")
        
        # Basic validation
        is_valid = len(query.strip()) > 3
        confidence = 0.9 if is_valid else 0.3
        
        return {
            "is_valid": is_valid,
            "confidence": confidence,
            "suggestions": self.generate_suggestions(query) if not is_valid else [],
            "query_type": self.classify_query_type(query)
        }
    
    def extract_region(self, query: str) -> Dict[str, Any]:
        """Extract geographic region from query"""
        query_lower = query.lower()
        
        regions = {
            "atlantic canada": {"north": 60, "south": 43, "east": -50, "west": -70, "name": "Atlantic Canada"},
            "north atlantic": {"north": 45, "south": 35, "east": -60, "west": -80, "name": "North Atlantic"},
            "south atlantic": {"north": 0, "south": -40, "east": -20, "west": -60, "name": "South Atlantic"},
            "pacific": {"north": 60, "south": -60, "east": -120, "west": 120, "name": "Pacific Ocean"},
            "equator": {"north": 10, "south": -10, "east": 180, "west": -180, "name": "Equatorial Region"},
            "gulf stream": {"north": 45, "south": 25, "east": -60, "west": -80, "name": "Gulf Stream"}
        }
        
        for key, region in regions.items():
            if key in query_lower:
                return region
        
        # Default to North Atlantic
        return regions["north atlantic"]
    
    def extract_parameters(self, query: str) -> List[str]:
        """Extract ocean parameters from query"""
        query_lower = query.lower()
        parameters = []
        
        if "temperature" in query_lower or "temp" in query_lower:
            parameters.append("temperature")
        if "salinity" in query_lower or "salt" in query_lower:
            parameters.append("salinity")
        if "pressure" in query_lower:
            parameters.append("pressure")
        if "current" in query_lower or "velocity" in query_lower:
            parameters.append("current")
        if "oxygen" in query_lower:
            parameters.append("oxygen")
        if "chlorophyll" in query_lower:
            parameters.append("chlorophyll")
        
        # Default to temperature and salinity
        if not parameters:
            parameters = ["temperature", "salinity"]
        
        return parameters
    
    def extract_time_range(self, query: str) -> Dict[str, str]:
        """Extract time range from query"""
        # Default to last year
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)
        
        return {
            "start": start_date.strftime("%Y-%m-%d"),
            "end": end_date.strftime("%Y-%m-%d")
        }
    
    async def fetch_argovis_data(self, region: Dict[str, Any], parameters: List[str], time_range: Dict[str, str]) -> Dict[str, Any]:
        """Fetch data from Argovis API"""
        try:
            url = f"{self.argovis_base_url}"
            params = {
                "startDate": f"{time_range['start']}T00:00:00.000Z",
                "endDate": f"{time_range['end']}T23:59:59.999Z",
                "polygon": json.dumps([
                    [region["west"], region["south"]],
                    [region["east"], region["south"]],
                    [region["east"], region["north"]],
                    [region["west"], region["north"]],
                    [region["west"], region["south"]]
                ])
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # Transform data to our format
            profiles = []
            for item in data:
                if "data" in item:
                    for profile in item["data"]:
                        profiles.append({
                            "id": f"argo_{item.get('_id', 'unknown')}",
                            "lat": item.get("lat", 0),
                            "lon": item.get("lon", 0),
                            "depth": profile.get("pres", 0),
                            "temperature": profile.get("temp", None),
                            "salinity": profile.get("psal", None),
                            "pressure": profile.get("pres", None),
                            "date": item.get("date", datetime.now().isoformat())
                        })
            
            return {
                "profiles": profiles,
                "metadata": {
                    "totalProfiles": len(profiles),
                    "dateRange": time_range,
                    "region": region["name"],
                    "source": "Argovis"
                },
                "isEmpty": len(profiles) == 0
            }
            
        except Exception as e:
            logger.error(f"Error fetching Argovis data: {e}")
            return {"profiles": [], "metadata": {}, "isEmpty": True}
    
    def generate_analysis(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """Generate intelligent analysis of ocean data"""
        profiles = data.get("profiles", [])
        
        if not profiles:
            return {
                "summary": "No data available for the specified region and time period.",
                "insights": [],
                "recommendations": ["Try expanding the time range or geographic area"]
            }
        
        # Basic statistics
        temps = [p.get("temperature") for p in profiles if p.get("temperature") is not None]
        salts = [p.get("salinity") for p in profiles if p.get("salinity") is not None]
        
        analysis = {
            "summary": f"Analysis of {len(profiles)} ocean profiles in the specified region",
            "statistics": {
                "profile_count": len(profiles),
                "temperature_range": [min(temps), max(temps)] if temps else None,
                "salinity_range": [min(salts), max(salts)] if salts else None,
                "mean_temperature": np.mean(temps) if temps else None,
                "mean_salinity": np.mean(salts) if salts else None
            },
            "insights": [
                f"Found {len(profiles)} ocean profiles",
                f"Temperature range: {min(temps):.1f}Â°C to {max(temps):.1f}Â°C" if temps else "No temperature data",
                f"Salinity range: {min(salts):.1f} to {max(salts):.1f} PSU" if salts else "No salinity data"
            ],
            "recommendations": [
                "Consider visualizing the data as a 3D scatter plot",
                "Analyze temperature-depth relationships",
                "Look for seasonal patterns in the data"
            ]
        }
        
        return analysis
    
    def generate_suggestions(self, query: str) -> List[str]:
        """Generate query suggestions"""
        return [
            "Try asking about temperature data in the North Atlantic",
            "Ask for salinity patterns near the equator",
            "Request ocean current analysis in the Gulf Stream"
        ]
    
    def classify_query_type(self, query: str) -> str:
        """Classify the type of ocean query"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["temperature", "temp", "salinity", "salt"]):
            return "data_analysis"
        elif any(word in query_lower for word in ["current", "flow", "circulation"]):
            return "current_analysis"
        elif any(word in query_lower for word in ["anomaly", "trend", "change"]):
            return "trend_analysis"
        else:
            return "general_inquiry"

# Create Flask app
app = Flask(__name__)
CORS(app)

# Initialize server
mcp_server = OceanMCPServer()

@app.route('/api/mcp/intelligent_ocean_query', methods=['POST'])
def intelligent_ocean_query():
    """Handle intelligent ocean queries via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("intelligent_ocean_query", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in intelligent_ocean_query: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/get_real_time_metrics', methods=['POST'])
def get_real_time_metrics():
    """Get real-time ocean metrics via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("get_real_time_metrics", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in get_real_time_metrics: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/validate_input', methods=['POST'])
def validate_input():
    """Validate input via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("validate_input", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in validate_input: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/analyze_map_selection', methods=['POST'])
def analyze_map_selection():
    """Analyze map selection via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("analyze_map_selection", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in analyze_map_selection: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/spatial_analysis', methods=['POST'])
def spatial_analysis():
    """Spatial analysis via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("spatial_analysis", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in spatial_analysis: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/temporal_analysis', methods=['POST'])
def temporal_analysis():
    """Temporal analysis via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("temporal_analysis", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in temporal_analysis: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/data_quality_assessment', methods=['POST'])
def data_quality_assessment():
    """Data quality assessment via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("data_quality_assessment", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in data_quality_assessment: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/mcp/enhance_data_quality', methods=['POST'])
def enhance_data_quality():
    """Data quality enhancement via HTTP"""
    try:
        data = request.get_json()
        result = asyncio.run(mcp_server.handle_request("enhance_data_quality", data))
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error in enhance_data_quality: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

def main():
    """Main server function"""
    logger.info("Starting FloatChat Ocean AI MCP Server on port 5001...")
    app.run(host='0.0.0.0', port=5001, debug=True)

if __name__ == "__main__":
    main()
